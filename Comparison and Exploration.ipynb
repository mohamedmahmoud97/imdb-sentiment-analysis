{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "% reset\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from helpers import *\n",
    "from cleaners import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import pathlib\n",
    "from multiprocessing import Process,Lock,Value\n",
    "from pathlib import Path\n",
    "from gensim.models import Word2Vec,TfidfModel,Phrases\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some helpers used to convert docs to vecs using tf weighted word2vec averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# an iterator used by gensim to train models without loading the whole corpus in memory\n",
    "class MyDocs(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        pathlist = Path(self.dirname).glob('**/*')\n",
    "        for path in pathlist:\n",
    "            path_in_str = str(path)\n",
    "            if os.path.isfile(path_in_str):\n",
    "                f=open(path_in_str)\n",
    "                yield f.read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2vec(docs_root,model_name=None):\n",
    "    # Training a word2vec model, the bigram transformer tells gensim to create vectors for bigram phrases such New York\n",
    "    save = model_name!=None\n",
    "    model_path = f'models/wv_{model_name}'\n",
    "    model_exists = os.path.isfile(model_path)\n",
    "    dir_exists = os.path.isdir('models')\n",
    "\n",
    "    wv_model = None\n",
    "    \n",
    "    if model_exists:\n",
    "        wv_model = Word2Vec.load(model_path)\n",
    "    else:\n",
    "        docs = MyDocs(docs_root)\n",
    "        wv_model = Word2Vec(docs, size=300, window=5, min_count=100, workers=6)\n",
    "    \n",
    "    if save:\n",
    "        if not dir_exists:\n",
    "            pathlib.Path.mkdir('models')\n",
    "        if not model_exists:\n",
    "            wv_model.save(model_path)\n",
    "\n",
    "    return wv_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_idf_model(doc_path,model_name=None):\n",
    "    model_path = f'models/tf_{model_name}'\n",
    "    dict_path = f'models/dict_{model_name}'\n",
    "    model_exists = os.path.isfile(model_path)\n",
    "    dict_exists = os.path.isfile(dict_path)\n",
    "    dir_exists = os.path.isdir('models')\n",
    "    save = model_name!=None\n",
    "    \n",
    "    docs = MyDocs(doc_path)\n",
    "    bow_dict = Dictionary.load(dict_path) if dict_exists else Dictionary(docs)\n",
    "    corpus = [bow_dict.doc2bow(doc) for doc in docs]\n",
    "    tf_model = TfidfModel.load(model_path) if model_exists else TfidfModel(corpus)\n",
    "    if save:\n",
    "        if not dir_exists:\n",
    "            pathlib.Path.mkdir('models')\n",
    "        if not model_exists:\n",
    "            tf_model.save(model_path)\n",
    "        if not dict_exists:\n",
    "            bow_dict.save(dict_path)\n",
    "    return corpus,bow_dict,tf_model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_doc2vec_factory(bow_dict,tf_model,word2vec):\n",
    "    def tf_doc2vec(doc_bow,raw_doc):\n",
    "        tf_dict = {e0:e1 for e0,e1 in tf_model[doc_bow]}\n",
    "        doc_tf_idf = np.array([tf_dict[tok_id] for tok_id in (bow_dict.token2id[tok] for tok in raw_doc if tok in word2vec.wv.vocab)])\n",
    "        vecs = np.array([word2vec.wv[word] for idx,word in enumerate(raw_doc) if word in word2vec.wv.vocab])\n",
    "        doc_vec = np.sum(doc_tf_idf.reshape(-1,1)*vecs,axis=0)\n",
    "        return doc_vec\n",
    "    return tf_doc2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_wv_samples(model_prefix,root_dir,wv=None):\n",
    "    # root dir is assumed to have children /pos and /neg\n",
    "    # word 2 vec model is created using all descendents of the parent of root_dir\n",
    "    # returns X,y \n",
    "    \n",
    "    pos_dir = f'{root_dir}/pos'\n",
    "    neg_dir = f'{root_dir}/neg'\n",
    "    \n",
    "    # get the word2vec model\n",
    "    if wv is None:\n",
    "        wv = word2vec(root_dir.split('/')[0],model_prefix)\n",
    "        \n",
    "    # get tf-idf models\n",
    "    pos_corpus, pos_dict, pos_tf_model = tf_idf_model(pos_dir, f'{model_prefix}_pos')\n",
    "    neg_corpus, neg_dict, neg_tf_model = tf_idf_model(neg_dir, f'{model_prefix}_neg')\n",
    "    # get tf-doc2vec factories\n",
    "    pos_tf_doc2vec = tf_doc2vec_factory(pos_dict, pos_tf_model, wv)\n",
    "    neg_tf_doc2vec = tf_doc2vec_factory(neg_dict, neg_tf_model, wv)\n",
    "    \n",
    "    pos_vecs = []\n",
    "    for bows,doc in zip(pos_corpus,MyDocs(pos_dir)):\n",
    "        pos_vecs.append(pos_tf_doc2vec(bows,doc))\n",
    "    pos_vecs = np.array(pos_vecs)\n",
    "    \n",
    "    neg_vecs = []\n",
    "    for bows,doc in zip(neg_corpus,MyDocs(neg_dir)):\n",
    "        neg_vecs.append(neg_tf_doc2vec(bows,doc))\n",
    "    neg_vecs = np.array(neg_vecs)\n",
    "    \n",
    "    X = np.concatenate((pos_vecs,neg_vecs),axis=0)\n",
    "    y = np.asarray([1]*pos_vecs.shape[0] + [-1]*pos_vecs.shape[0])\n",
    "    \n",
    "    return X, y, wv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleaning the data without POS tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  cleaning train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_cleaned_root = speedy(basic_cleaner, ['aclImdb/train/pos','aclImdb/train/neg'],10,target_suffix='clean_nltkTok_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y,wv_basic = tf_wv_samples(model_prefix='basic_nltk_2', root_dir=f'{basic_cleaned_root}/train')\n",
    "X_train,X_val,y_train,y_val = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### validation of the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(C=1)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72981818181818181"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(clf.predict(X_val) == y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_cleaned_test = speedy(basic_cleaner,['aclImdb/test/pos','aclImdb/test/neg'],10,target_suffix='clean_nltkTok_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test,y_test,wv_basic = tf_wv_samples(model_prefix='basic_nltk_2', root_dir=f'{basic_cleaned_test}/test',wv=wv_basic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73760000000000003"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(clf.predict(X_test) == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagged documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_cleaned_root = speedy(pos_cleaner, ['aclImdb/train/pos','aclImdb/train/neg','aclImdb/train/unsup'],15,target_suffix='pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y,wv_pos = tf_wv_samples(model_prefix='pos', root_dir=f'{pos_cleaned_root}/train')\n",
    "X_train,X_val,y_train,y_val = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(C=1)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78181818181818186"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(clf.predict(X_val) == y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basic_cleaned_test = speedy(basic_cleaner,['aclImdb/test/pos','aclImdb/test/neg'],10,target_suffix='clean_nltkTok_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google pre-trained word2vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alyswidan/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)  \n",
    "basic_cleaned_root = speedy(basic_cleaner, ['aclImdb/train/pos','aclImdb/train/neg','aclImdb/train/unsup'],15,target_suffix='clean')\n",
    "X,y = tf_wv_samples(model_prefix='google', root_dir=f'{basic_cleaned_root}/train')\n",
    "X_train,X_val,y_train,y_val = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7781818181818182"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(C=1)\n",
    "clf.fit(X_train, y_train)\n",
    "np.mean(clf.predict(X_val) == y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
