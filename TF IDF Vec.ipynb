{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOLA AMIGOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "% reset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from helpers import *\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import pathlib\n",
    "from multiprocessing import Process,Lock,Value\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import SVC\n",
    "import ntpath\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ntpath.basename(\"a/b/c/d/e\")\n",
    "\n",
    "def path_leaf(path):\n",
    "    a,b,c,d,e = path.split(\"/\")\n",
    "    return d,e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs_train = ['Data/aclImdb/train/pos','Data/aclImdb/train/neg']\n",
    "clean_root_train = speedy(dirs_train,15)\n",
    "dirs_test = ['Data/aclImdb/test/pos','Data/aclImdb/test/neg']\n",
    "clean_root_test = speedy(dirs_test,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "class MyDocs(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "        self.pos = []\n",
    "        self.neg = []\n",
    "        self.loopy()\n",
    " \n",
    "    def __iter__(self):\n",
    "        self.loopy()\n",
    "    \n",
    "    def loopy(self):\n",
    "        pathlist = Path(self.dirname).glob('**/*')\n",
    "        for path in pathlist:\n",
    "            path_in_str = str(path)\n",
    "            if os.path.isfile(path_in_str):\n",
    "                f=open(path_in_str)\n",
    "                parent, uid = path_leaf(path_in_str)\n",
    "                uid, _ = uid.split(\"_\")\n",
    "                uid = int(str(uid))\n",
    "                if parent == \"pos\":\n",
    "                    self.pos.append(f.read())\n",
    "                elif parent == \"neg\":\n",
    "                    self.neg.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs_train = MyDocs(clean_root_train)\n",
    "docs_test = MyDocs(clean_root_test)\n",
    "# bigram_transformer = Phrases(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier,LogisticRegressionCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "pos_train = np.array(docs_train.pos)\n",
    "neg_train = np.array(docs_train.neg)\n",
    "X_train = np.concatenate((pos_train,neg_train),axis=0)\n",
    "y_train = np.asarray([1]*pos_train.shape[0] + [-1]*neg_train.shape[0])\n",
    "\n",
    "pos_test = np.array(docs_test.pos)\n",
    "neg_test = np.array(docs_test.neg)\n",
    "X_test = np.concatenate((pos_test,neg_test),axis=0)\n",
    "y_test = np.asarray([1]*pos_test.shape[0] + [-1]*neg_test.shape[0])\n",
    "\n",
    "# X_train,X_val,y_train,y_val = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42,max_iter=5, tol=None)),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85274000000000005"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(X_train, y_train)\n",
    "np.mean(text_clf.predict(X_test) == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),('clf', MultinomialNB()),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89966000000000002"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(X_train, y_train)\n",
    "np.mean(text_clf.predict(X_test) == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),('clf', LogisticRegressionCV(solver='lbfgs')),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95098000000000005"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(X_train, y_train)\n",
    "np.mean(text_clf.predict(X_test) == y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
