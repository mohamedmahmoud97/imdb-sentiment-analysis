{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pure TF-IDF Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "% reset\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import ntpath\n",
    "import pathlib\n",
    "\n",
    "from helpers.cleaner_TFIDF import *\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "from multiprocessing import Process,Lock,Value\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier,LogisticRegressionCV,LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ntpath.basename(\"a/b/c/d/e\")\n",
    "\n",
    "def path_leaf(path):\n",
    "    a,b,c,d,e = path.split(\"/\")\n",
    "    return d,e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f71c1c259ecf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdirs_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Data/aclImdb/train/pos'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Data/aclImdb/train/neg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclean_root_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspeedy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirs_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdirs_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Data/aclImdb/test/pos'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Data/aclImdb/test/neg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclean_root_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspeedy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirs_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspaces/IMDB Sentiment Analysis/helpers/cleaner_TFIDF.py\u001b[0m in \u001b[0;36mspeedy\u001b[0;34m(cleaner, dirs, n_jobs, target_suffix)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprocs_per_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mcurr_dir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mcurr_dir_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdir_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtarget_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34mf'_{target_suffix}/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "dirs_train = ['Data/aclImdb/train/pos','Data/aclImdb/train/neg']\n",
    "clean_root_train = speedy(dirs_train,15)\n",
    "dirs_test = ['Data/aclImdb/test/pos','Data/aclImdb/test/neg']\n",
    "clean_root_test = speedy(dirs_test,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "class MyDocs(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "        self.pos = []\n",
    "        self.neg = []\n",
    "        self.loopy()\n",
    " \n",
    "    def __iter__(self):\n",
    "        self.loopy()\n",
    "    \n",
    "    def loopy(self):\n",
    "        pathlist = Path(self.dirname).glob('**/*')\n",
    "        for path in pathlist:\n",
    "            path_in_str = str(path)\n",
    "            if os.path.isfile(path_in_str):\n",
    "                f=open(path_in_str)\n",
    "                parent, uid = path_leaf(path_in_str)\n",
    "                uid, _ = uid.split(\"_\")\n",
    "                uid = int(str(uid))\n",
    "                if parent == \"pos\":\n",
    "                    self.pos.append(f.read())\n",
    "                elif parent == \"neg\":\n",
    "                    self.neg.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs_train = MyDocs(clean_root_train)\n",
    "docs_test = MyDocs(clean_root_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_train = np.array(docs_train.pos)\n",
    "neg_train = np.array(docs_train.neg)\n",
    "X_train = np.concatenate((pos_train,neg_train),axis=0)\n",
    "y_train = np.asarray([1]*pos_train.shape[0] + [-1]*neg_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_test = np.array(docs_test.pos)\n",
    "neg_test = np.array(docs_test.neg)\n",
    "X_test = np.concatenate((pos_test,neg_test),axis=0)\n",
    "y_test = np.asarray([1]*pos_train.shape[0] + [-1]*neg_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning and crossvalidating the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val = sklearn.cross_validation.train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeliner = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),('clf', LogisticRegression(solver='lbfgs')),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeliner.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pg = [\n",
    "    {\n",
    "        'clf': [LinearSVC()],\n",
    "        'clf__C_lr': [0.1,5,10,15,20]\n",
    "    },\n",
    "    {\n",
    "        'clf': [RandomForestClassifier()],\n",
    "        'clf__n_trees': [50,100,200],\n",
    "        'clf__n_jobs': [-1]\n",
    "    },\n",
    "    {\n",
    "        'clf': [KNeighborsClassifier()],\n",
    "        'clf__n_neighbours': [3,5,11,13]\n",
    "    },\n",
    "    {\n",
    "        'clf': [SVC()],\n",
    "        'clf__svm_C': [0.1, 1, 5,10]\n",
    "    },\n",
    "    {\n",
    "        'clf': [DecisionTreeClassifier()],\n",
    "        'clf__min_samples_split': [2, 10, 20]\n",
    "    },\n",
    "    {\n",
    "        'clf': [LogisticRegression()],\n",
    "        'clf__C_lr' = [0.1,5,10,15,20]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(pipeliner, param_grid=pg, cv=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "y_pred = full_pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42,max_iter=5, tol=None)),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85274000000000005"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(X_train, y_train)\n",
    "text_clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),('clf', MultinomialNB()),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89966000000000002"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(X_train, y_train)\n",
    "text_clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),('clf', LogisticRegression(solver='lbfgs')),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92874000000000001"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(X_train, y_train)\n",
    "text_clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),('clf', LogisticRegressionCV(solver='lbfgs')),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95098000000000005"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(X_train, y_train)\n",
    "text_clf.score(X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
