{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOLA AMIGOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -rf datalab\n",
    "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz /Here_the_directory_you_want_to_put_in\n",
    "!uncompress aclImdb_v1.tar.gz\n",
    "!tar -xvf aclImdb_v1.tar\n",
    "!rm aclImdb_v1.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "% reset\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import pathlib\n",
    "from multiprocessing import Process\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "    \n",
    "def dir_size(src_dir):\n",
    "    return len([name for name in os.listdir(os.fsencode(src_dir))])\n",
    "def write_clean_docs(src_dir,start,end):\n",
    "    current_doc = None\n",
    "    cnt=0\n",
    "    total = len([name for name in os.listdir(os.fsencode(src_dir))])\n",
    "    directory = os.fsencode(src_dir)\n",
    "    target_dir = src_dir.split('/')[0]+'_clean/'+'/'.join(src_dir.split('/')[1:])\n",
    "    pathlib.Path(target_dir).mkdir(parents=True, exist_ok=True)\n",
    "    for file in os.listdir(directory):\n",
    "        filename = os.fsdecode(file)\n",
    "        if( start <= int(filename.split('_')[0]) <= end):\n",
    "            with open(os.path.join(src_dir,filename),'r') as f:\n",
    "                current_doc = f.read()\n",
    "            with open(os.path.join(target_dir,filename),'w') as f:\n",
    "                f.write(clean_doc(current_doc))\n",
    "            cnt += 1\n",
    "            print(f'cleaning {src_dir} : cleaned {cnt}/{total} files')\n",
    "            \n",
    "def read_and_clean(src_dir):\n",
    "    clean_docs = []\n",
    "    for file in os.listdir(os.fsencode(src_dir)):\n",
    "        filename =  os.fsdecode(file)\n",
    "        with open(os.path.join(src_dir,filename)) as f:\n",
    "            clean_docs.append(clean_doc(f.read()))\n",
    "    return clean_docs\n",
    "        \n",
    "\n",
    "def clean_doc(doc):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    clean_doc = re.sub(r'<.+?>|[!\"#$%&\\'()=*+,-./:;?@\\[\\]^_`{|}~<>]|[0-9]', ' ', doc)\n",
    "    clean_doc = ' '.join([word.lower() for word in clean_doc.split()])\n",
    "    tokens =  [word for word in clean_doc.split() if word not in set(stopwords.words('english'))]\n",
    "    tagged_tokens = [(pair[0],get_wordnet_pos(pair[1])) for pair in nltk.pos_tag(tokens)]\n",
    "    clean_doc = ' '.join([ lemmatizer.lemmatize(word,tag) for word,tag in tagged_tokens])\n",
    "    return clean_doc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 448\n",
      "448 896\n",
      "896 1344\n",
      "1344 1792\n",
      "1792 2240\n",
      "2240 2688\n"
     ]
    }
   ],
   "source": [
    "write_clean_docs('aclImdb/train/pos')\n",
    "write_clean_docs('aclImdb/train/neg')\n",
    "write_clean_docs('aclImdb/train/unsup')\n",
    "dirs = [('data_subset/1',0,4),('data_subset/1',5,10),('data_subset/1',11,15),('data_subset/2',0,4),('data_subset/2',5,10),('data_subset/2',11,15)]\n",
    "procs = []\n",
    "max_conc = \n",
    "for (curr_dir,s,e) in dirs:\n",
    "    proc = Process(target=write_clean_docs, args=(curr_dir,s,e,))\n",
    "    procs.append(proc)\n",
    "    proc.start()\n",
    "for proc in procs:\n",
    "    proc.join()\n",
    "size = dir_size('data_subset/1')\n",
    "n_threads = 6\n",
    "for i in range(n_threads):\n",
    "    print(i*size//n_threads,(i+1)*size//n_threads)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "from pathlib import Path\n",
    "import os\n",
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        pathlist = Path(self.dirname).glob('**/*')\n",
    "        for path in pathlist:\n",
    "            path_in_str = str(path)\n",
    "            if os.path.isfile(path_in_str):\n",
    "                for line in open(path_in_str):\n",
    "                    yield [w.lower() for w in line.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyDocs(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        pathlist = Path(self.dirname).glob('**/*')\n",
    "        for path in pathlist:\n",
    "            path_in_str = str(path)\n",
    "            if os.path.isfile(path_in_str):\n",
    "                f=open(path_in_str)\n",
    "                yield [w.lower() for w in f.read().split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "sentence = MySentences('Data/cleaned_data/train/pos')\n",
    "\n",
    "wv_model = Word2Vec(sentence, size=100, window=5, min_count=100, workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = MyDocs('Data/cleaned_data/train/pos')\n",
    "dct = Dictionary(docs)\n",
    "corpus = [dct.doc2bow(line) for line in docs]\n",
    "tf_model = TfidfModel(corpus)\n",
    "vector = tf_model[corpus[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_corpus = [doc for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamedmahmoud/myprograms/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('fantastic', 0.8176398277282715),\n",
       " ('hilarious', 0.7408750057220459),\n",
       " ('amazing', 0.7405626773834229),\n",
       " ('ok', 0.7380691170692444),\n",
       " ('incredible', 0.7281054854393005),\n",
       " ('cool', 0.7279859781265259),\n",
       " ('absolutely', 0.7059080004692078),\n",
       " ('okay', 0.6849677562713623),\n",
       " ('plus', 0.6837688088417053),\n",
       " ('fabulous', 0.6790148019790649)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_model.similar_by_word(\"awesome\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
