{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOLA AMIGOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "% reset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from helpers import *\n",
    "from cleaners import *\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import pathlib\n",
    "from multiprocessing import Process,Lock,Value\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaningaclImdb/train/unsup\n"
     ]
    }
   ],
   "source": [
    "dirs = ['aclImdb/train/pos','aclImdb/train/neg','aclImdb/train/unsup']\n",
    "clean_root = speedy(pos_cleaner, dirs,15,target_suffix='pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "class MyDocs(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        pathlist = Path(self.dirname).glob('**/*')\n",
    "        for path in pathlist:\n",
    "            path_in_str = str(path)\n",
    "            if os.path.isfile(path_in_str):\n",
    "                f=open(path_in_str)\n",
    "                yield f.read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alyswidan/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec,TfidfModel,Phrases\n",
    "from gensim.corpora import Dictionary\n",
    "import gensim\n",
    "\n",
    "docs = MyDocs(clean_root)\n",
    "bigram_transformer = Phrases(docs)\n",
    "wv_model = Word2Vec(bigram_transformer[docs], size=300, window=5, min_count=100, workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_idf_model(doc_path,model_name=None):\n",
    "    model_path = f'models/tf_{model_name}'\n",
    "    dict_path = f'models/dict_{model_name}'\n",
    "    model_exists = os.path.isfile(model_path)\n",
    "    dict_exists = os.path.isfile(dict_path)\n",
    "    dir_exists = os.path.isdir('models')\n",
    "    save = model_name!=None\n",
    "    \n",
    "    docs = MyDocs(doc_path)\n",
    "    bow_dict = Dictionary.load(dict_path) if dict_exists else Dictionary(docs)\n",
    "    corpus = [bow_dict.doc2bow(doc) for doc in docs]\n",
    "    tf_model = TfidfModel.load(model_path) if model_exists else TfidfModel(corpus)\n",
    "    if save:\n",
    "        if not dir_exists:\n",
    "            pathlib.Path.mkdir('models')\n",
    "        if not model_exists:\n",
    "            tf_model.save(model_path)\n",
    "        if not dict_exists:\n",
    "            bow_dict.save(dict_path)\n",
    "    return corpus,bow_dict,tf_model\n",
    "        \n",
    "\n",
    "pos_corpus, pos_dict, pos_tf_model = tf_idf_model(f'{clean_root}/train/pos','pos')\n",
    "neg_corpus, neg_dict, neg_tf_model = tf_idf_model(f'{clean_root}/train/neg','neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.021976807651603317,\n",
       " 0.022925812720642178,\n",
       " 0.070406607929402329,\n",
       " 0.09951364440068855,\n",
       " 0.050145578715350608,\n",
       " 0.11540032501294552,\n",
       " 0.10135678257370058,\n",
       " 0.079098294430211241,\n",
       " 0.1131214593145272,\n",
       " 0.067350853376543673,\n",
       " 0.098651363245904899,\n",
       " 0.074836207845630845,\n",
       " 0.071110856552041316,\n",
       " 0.10750955500910521,\n",
       " 0.10260015112089857,\n",
       " 0.088945001706924334,\n",
       " 0.10957172827794498,\n",
       " 0.10993679309219441,\n",
       " 0.092822766510904361,\n",
       " 0.056974249706613633,\n",
       " 0.055492060740391341,\n",
       " 0.090862971171037865,\n",
       " 0.033562197564937016,\n",
       " 0.10316393397165001,\n",
       " 0.11588855463830307,\n",
       " 0.024087506740569888,\n",
       " 0.097029654042302099,\n",
       " 0.090296205570073701,\n",
       " 0.10752524667042938,\n",
       " 0.08308375299219975,\n",
       " 0.12290430131149949,\n",
       " 0.055693659906542618,\n",
       " 0.10316393397165001,\n",
       " 0.024935215704308161,\n",
       " 0.076252663130828763,\n",
       " 0.1131214593145272,\n",
       " 0.074258213208723486,\n",
       " 0.049040013306127464,\n",
       " 0.053556465602493691,\n",
       " 0.12435209915515089,\n",
       " 0.067714281040856153,\n",
       " 0.10752524667042938,\n",
       " 0.062209120691583839,\n",
       " 0.071110856552041316,\n",
       " 0.11638884052645829,\n",
       " 0.071110856552041316,\n",
       " 0.01136610197897754,\n",
       " 0.079178853379421835,\n",
       " 0.052147999478434799,\n",
       " 0.12011400066293169,\n",
       " 0.071110856552041316,\n",
       " 0.011637752673019006,\n",
       " 0.05420485850487463,\n",
       " 0.047344092118740702,\n",
       " 0.11227714760574065,\n",
       " 0.081323203907569822,\n",
       " 0.080686531713044518,\n",
       " 0.12155309744835015,\n",
       " 0.040090844519681298,\n",
       " 0.12011400066293169,\n",
       " 0.06030986538015868,\n",
       " 0.069787917138309258,\n",
       " 0.081323203907569822,\n",
       " 0.070776432651822027,\n",
       " 0.082986111603057175,\n",
       " 0.047597246795491814,\n",
       " 0.058111690741559877,\n",
       " 0.028746022598769932,\n",
       " 0.10817389482221389,\n",
       " 0.022925812720642178,\n",
       " 0.085470101961443598]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc0 = raw_docs[11]\n",
    "bow0 = bows[11]\n",
    "pos_tf_model[bow0]\n",
    "tf_dict = {e0:e1 for e0,e1 in pos_tf_model[bow0]}\n",
    "sorted_tfs = [tf_dict[tok_id] for tok_id in (pos_dict.token2id[tok] for tok in doc0 if tok in wv_model.wv.vocab)]\n",
    "sorted_tfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doc2vec_factory(bow_dict,tf_model,word2vec):\n",
    "    def doc2vec(doc_bow,raw_doc):\n",
    "        tf_dict = {e0:e1 for e0,e1 in pos_tf_model[doc_bow]}\n",
    "        doc_tf_idf = np.array([tf_dict[tok_id] for tok_id in (bow_dict.token2id[tok] for tok in raw_doc if tok in word2vec.wv.vocab)])\n",
    "        vecs = np.array([word2vec.wv[word] for idx,word in enumerate(raw_doc) if word in word2vec.wv.vocab])\n",
    "        doc_vec = np.sum(doc_tf_idf.reshape(-1,1)*vecs,axis=0)\n",
    "        return doc_vec.reshape(-1,1)\n",
    "    return doc2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_doc2vec = doc2vec_factory(pos_dict,pos_tf_model,wv_model)\n",
    "neg_doc2vec = doc2vec_factory(neg_dict,neg_tf_model,wv_model)\n",
    "\n",
    "pos_vecs = []\n",
    "for bows,doc in zip(pos_corpus,MyDocs(f'{clean_root}/train/pos')):\n",
    "    pos_vecs.append(pos_doc2vec(bows,doc))\n",
    "pos_vecs = np.array(pos_vecs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_vecs = []\n",
    "for bows,doc in zip(neg_corpus,MyDocs(f'{clean_root}/train/neg')):\n",
    "    neg_vecs.append(neg_doc2vec(bows,doc))\n",
    "neg_vecs = np.array(neg_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_vecs = neg_vecs.reshape(-1,300)\n",
    "pos_vecs = pos_vecs.reshape(-1,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.concatenate((pos_vecs,neg_vecs),axis=0)\n",
    "y = np.asarray([1]*pos_vecs.shape[0] + [-1]*pos_vecs.shape[0])\n",
    "\n",
    "X_train,X_val,y_train,y_val = train_test_split(X, y, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(C=1)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88145454545454549"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(clf.predict(X_val) == y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=50,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators=500,min_samples_split=50,n_jobs=-1)\n",
    "forest.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83624242424242423"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(forest.predict(X_val) == y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=200, random_state=None)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada = AdaBoostClassifier(n_estimators=200)\n",
    "ada.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85224242424242425"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(ada.predict(X_val) == y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=300,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gboost = GradientBoostingClassifier(n_estimators=300)\n",
    "gboost.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86896969696969695"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(gboost.predict(X_val) == y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
